{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Acknowledgements:\n",
    "1. We referenced the skeleton code of Assignment 7, the original SAC paper (Haarnoja et. al., 2018, https://arxiv.org/abs/1812.05905), \n",
    "and the PyTorch implementation of Discor (https://github.com/toshikwa/discor.pytorch) to implement the SAC algorithm.\n",
    "2. We referenced the original BRO paper (Nauman et. al., 2024, https://arxiv.org/abs/2405.16158), its jax implementation \n",
    "(https://github.com/naumix/BiggerRegularizedOptimistic), and its PyTorch implementation (https://github.com/naumix/BiggerRegularizedOtimistic_Torch)\n",
    "to build upon our SAC implementation and implement the BRO algorithm. \n",
    "We also note that the authors' PyTorch implementation of BRO is not coherent with the original paper or its jax implementation, \n",
    "but is rather an over-simplified version of the jax implementation.\n",
    "3. We did NOT directly use the aforementioned open-sourced implementations of SAC and BRO for our implementation,\n",
    "although they provided us with insights and hints for debugging purposes. \n",
    "We implemented the neural network classes and the update function ourself.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170085,
     "status": "ok",
     "timestamp": 1746410108019,
     "user": {
      "displayName": "Donggeon Oh (David)",
      "userId": "03001976725357808761"
     },
     "user_tz": 240
    },
    "id": "6ef69221",
    "outputId": "b67f91f9-05f2-4039-9e51-543d1f430104"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade \"gymnasium[mujoco]\" torch torchvision torchaudio tqdm matplotlib \"gymnasium[other]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2022,
     "status": "ok",
     "timestamp": 1746410110044,
     "user": {
      "displayName": "Donggeon Oh (David)",
      "userId": "03001976725357808761"
     },
     "user_tz": 240
    },
    "id": "2d846ab8",
    "outputId": "e11824f9-b8df-4950-c2ec-fe49a3674a6e"
   },
   "outputs": [],
   "source": [
    "import os, math, random, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------  EXPERIMENT  SWITCH-BOARD  ------------------\n",
    "LARGE_SCALE   = False\n",
    "USE_CDQ       = True\n",
    "REPLAY_RATIO  = 1\n",
    "DUAL_ACTORS   = False\n",
    "USE_QUANTILE  = False\n",
    "USE_WD        = False\n",
    "HARD_RESET    = False\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# ------------ reproducibility ------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('is_available:', torch.cuda.is_available())\n",
    "print('PyTorch built for CUDA', torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print('runtime driver version', torch.cuda.get_device_properties(0).major,\n",
    "          '.', torch.cuda.get_device_properties(0).minor, sep='')\n",
    "    print('device:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# ------------ environment ---------------\n",
    "ENV_ID = \"Ant-v5\"\n",
    "env    = gym.make(ENV_ID); env.reset(seed=SEED)\n",
    "OBS_DIM = env.observation_space.shape[0]\n",
    "ACT_DIM = env.action_space.shape[0]\n",
    "ACT_MAX = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1746410110087,
     "user": {
      "displayName": "Donggeon Oh (David)",
      "userId": "03001976725357808761"
     },
     "user_tz": 240
    },
    "id": "ff95c9c2"
   },
   "outputs": [],
   "source": [
    "CRITIC_WID, CRITIC_DEPTH = 512, 3\n",
    "ACTOR_WID, ACTOR_DEPTH = 256, 2\n",
    "CRITIC_MLP_WID           = 256\n",
    "ACTOR_MLP_WID           = 256\n",
    "N_QUANT           = 100 if USE_QUANTILE else 1\n",
    "BATCH             = 128\n",
    "UTD               = REPLAY_RATIO\n",
    "LR                = 3e-4\n",
    "TAU               = 0.005\n",
    "GAMMA             = 0.99\n",
    "LOG_STD_MIN, LOG_STD_MAX = -5, 2\n",
    "TARGET_ENT        = -ACT_DIM/2\n",
    "WD_COEFF          = 1e-4 if USE_WD else 0.0\n",
    "GRAD_CLIP         = 10.0\n",
    "RESET_EVERY       = 50_000\n",
    "TOTAL_STEPS       = 500_000\n",
    "EVAL_EVERY        = 5_000\n",
    "START_RANDOM      = 2_500\n",
    "EVAL_EPISODES     = 50\n",
    "KAPPA             = 1.0\n",
    "ALPHA_INIT        = 1.0\n",
    "\n",
    "def mlp(in_dim, out_dim, hidden=128):\n",
    "    return nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "                         nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "                         nn.Linear(hidden, out_dim))\n",
    "\n",
    "# residual block used by BRO\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, width: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(width, width, bias=True),\n",
    "            nn.LayerNorm(width),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(width, width, bias=True),\n",
    "            nn.LayerNorm(width)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# large BRO critic with quantile head\n",
    "class BroCriticLarge(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, CRITIC_WID),\n",
    "            nn.LayerNorm(CRITIC_WID),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.body = nn.Sequential(*[Residual(CRITIC_WID) for _ in range(CRITIC_DEPTH - 1)])\n",
    "        self.out  = nn.Linear(CRITIC_WID, N_QUANT)\n",
    "        taus = (torch.arange(N_QUANT)+0.5)/N_QUANT\n",
    "        self.register_buffer(\"taus\", taus.view(1,N_QUANT))\n",
    "    def forward(self,s,a):\n",
    "        h = self.inp(torch.cat([s, a], dim=-1))\n",
    "        h = self.body(h)\n",
    "        return self.out(h)\n",
    "    def optimistic(self, s, a, p: float = 0.84):\n",
    "        q = self.forward(s, a)\n",
    "        if q.shape[-1] == 1:\n",
    "            return q\n",
    "        idx = int(p * q.shape[-1])\n",
    "        return q[:, idx:idx + 1]\n",
    "    def mean(self, s, a):\n",
    "        q = self.forward(s, a)\n",
    "        return q.mean(-1, keepdim=True)\n",
    "\n",
    "# small BRO critic with quantile head\n",
    "class CriticSmall(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = mlp(obs_dim+act_dim, N_QUANT, CRITIC_MLP_WID)\n",
    "        self.register_buffer(\"taus\", torch.ones(1,N_QUANT)/N_QUANT)\n",
    "    def forward(self,s,a):\n",
    "        return self.net(torch.cat([s,a],-1))\n",
    "    def optimistic(self, s, a, p: float = 0.84):\n",
    "        q = self.forward(s, a)\n",
    "        if q.shape[-1] == 1:\n",
    "            return q\n",
    "        idx = int(p * q.shape[-1])\n",
    "        return q[:, idx:idx + 1]\n",
    "    def mean(self, s, a):\n",
    "        q = self.forward(s, a)\n",
    "        return q.mean(-1, keepdim=True)\n",
    "\n",
    "Critic = BroCriticLarge if LARGE_SCALE else CriticSmall\n",
    "\n",
    "# actor\n",
    "class BroActorLarge(nn.Module):\n",
    "    \"\"\"BroNet style stochastic policy head.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(obs_dim, ACTOR_WID),\n",
    "            nn.LayerNorm(ACTOR_WID),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *[Residual(ACTOR_WID) for _ in range(ACTOR_DEPTH-1)]\n",
    "        )\n",
    "        self.mu_head   = nn.Linear(ACTOR_WID, act_dim)\n",
    "        self.log_head  = nn.Linear(ACTOR_WID, act_dim)\n",
    "\n",
    "    def forward(self, s, det=False, logp=True):\n",
    "        h  = self.trunk(s)\n",
    "        mu = self.mu_head(h)\n",
    "        log_std = torch.clamp(self.log_head(h), LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        x = mu if det else dist.rsample()\n",
    "        y = torch.tanh(x)\n",
    "        lp = None\n",
    "        if logp:\n",
    "            lp = dist.log_prob(x).sum(-1,keepdim=True)\n",
    "            lp -= (2*(math.log(2)-x-F.softplus(-2*x))).sum(-1,keepdim=True)\n",
    "        return y, lp\n",
    "\n",
    "class ActorSmall(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = mlp(obs_dim, act_dim*2, ACTOR_MLP_WID)\n",
    "    def forward(self, s, det=False, logp=True):\n",
    "        mu, log_std = self.net(s).chunk(2,-1)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu,std)\n",
    "        x = mu if det else dist.rsample()\n",
    "        y = torch.tanh(x)\n",
    "        lp = None\n",
    "        if logp:\n",
    "            lp = dist.log_prob(x).sum(-1,keepdim=True)\n",
    "            lp -= (2*(math.log(2)-x-F.softplus(-2*x))).sum(-1,keepdim=True)\n",
    "        return y, lp\n",
    "\n",
    "Actor = BroActorLarge if LARGE_SCALE else ActorSmall\n",
    "\n",
    "# QR‑Huber loss\n",
    "def qhuber(pred,target,taus):\n",
    "    if N_QUANT==1:\n",
    "        return F.mse_loss(pred, target)\n",
    "    u = target.unsqueeze(2) - pred.unsqueeze(1)\n",
    "    hub = torch.where(u.abs()<=KAPPA, 0.5*u**2, KAPPA*(u.abs()-0.5*KAPPA))\n",
    "    loss = (taus.unsqueeze(0) - (u<0).float()).abs()*hub/KAPPA\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746410110095,
     "user": {
      "displayName": "Donggeon Oh (David)",
      "userId": "03001976725357808761"
     },
     "user_tz": 240
    },
    "id": "062c4fc6"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "Transition = namedtuple(\"T\", \"s a r s2 d\")\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, cap=int(2e6)):\n",
    "        self.buf = deque(maxlen=cap)\n",
    "    def add(self,*args): self.buf.append(Transition(*args))\n",
    "    def sample(self, n):\n",
    "        b = random.sample(self.buf, n)\n",
    "        b = Transition(*zip(*b))\n",
    "        to = lambda x: torch.as_tensor(x, dtype=torch.float32, device=device)\n",
    "        return (to(np.stack(b.s)),\n",
    "                to(np.stack(b.a)),\n",
    "                to(b.r).unsqueeze(1),\n",
    "                to(np.stack(b.s2)),\n",
    "                to(b.d).unsqueeze(1))\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "class BRO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ----- actors -----\n",
    "        if DUAL_ACTORS:\n",
    "            self.pi_p = Actor(OBS_DIM, ACT_DIM).to(device)\n",
    "            self.pi_o = Actor(OBS_DIM, ACT_DIM).to(device)\n",
    "        else:\n",
    "            self.pi = Actor(OBS_DIM, ACT_DIM).to(device)\n",
    "\n",
    "        # ----- critics ----\n",
    "        self.q1 = Critic(OBS_DIM, ACT_DIM).to(device)\n",
    "        self.q2 = Critic(OBS_DIM, ACT_DIM).to(device)\n",
    "        self.t1 = Critic(OBS_DIM, ACT_DIM).to(device); self.t1.load_state_dict(self.q1.state_dict())\n",
    "        self.t2 = Critic(OBS_DIM, ACT_DIM).to(device); self.t2.load_state_dict(self.q2.state_dict())\n",
    "        for p in (*self.t1.parameters(), *self.t2.parameters()): p.requires_grad=False\n",
    "\n",
    "        # ----- optimizers -----\n",
    "        opt_cls = torch.optim.Adam\n",
    "        self.opt_q = opt_cls([*self.q1.parameters(), *self.q2.parameters()], lr=LR, weight_decay=WD_COEFF)\n",
    "        if DUAL_ACTORS:\n",
    "            self.opt_p = opt_cls(self.pi_p.parameters(), lr=LR, weight_decay=WD_COEFF)\n",
    "            self.opt_o = opt_cls(self.pi_o.parameters(), lr=LR, weight_decay=WD_COEFF)\n",
    "        else:\n",
    "            self.opt_pi = opt_cls(self.pi.parameters(), lr=LR, weight_decay=WD_COEFF)\n",
    "\n",
    "        self.log_alpha = torch.tensor(math.log(ALPHA_INIT), requires_grad=True, device=device)\n",
    "        self.opt_alpha = opt_cls([self.log_alpha], lr=LR)\n",
    "\n",
    "        self.act_max = ACT_MAX\n",
    "        self.step_ctr= 0\n",
    "\n",
    "    # ---------- act ----------\n",
    "    @torch.no_grad()\n",
    "    def act(self,s,eval=False):\n",
    "        if DUAL_ACTORS:\n",
    "            # for evaluation, use the pessimistic actor\n",
    "            if eval:\n",
    "                a,_ = self.pi_p(torch.as_tensor(s,dtype=torch.float32,device=device).unsqueeze(0),\n",
    "                                det=eval, logp=False)\n",
    "            # for optimistic interaction with environment during training\n",
    "            else:\n",
    "                a,_ = self.pi_o(torch.as_tensor(s,dtype=torch.float32,device=device).unsqueeze(0),\n",
    "                                det=eval, logp=False)\n",
    "        else:\n",
    "            a,_ = self.pi(torch.as_tensor(s,dtype=torch.float32,device=device).unsqueeze(0),\n",
    "                          det=eval, logp=False)\n",
    "        return (a*self.act_max).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # ---------- update ----------\n",
    "    def update(self, batch):\n",
    "        s,a,r,s2,d = batch\n",
    "        alpha = self.log_alpha.exp()\n",
    "\n",
    "        # ---- critic target ----\n",
    "        with torch.no_grad():\n",
    "            if DUAL_ACTORS:\n",
    "                # Calculate critic target value using pessimistic actor\n",
    "                a2, lp2 = self.pi_p(s2)\n",
    "            else:\n",
    "                a2, lp2 = self.pi(s2)\n",
    "            q1_t = self.t1.optimistic(s2,a2)\n",
    "            q2_t = self.t2.optimistic(s2,a2)\n",
    "            q_t  = torch.min(q1_t,q2_t) if USE_CDQ else 0.5*(q1_t+q2_t)\n",
    "            tgt  = r + GAMMA*(1-d)*(q_t - alpha*lp2)\n",
    "\n",
    "        # ---- critic update ----\n",
    "        q1, q2 = self.q1(s,a), self.q2(s,a)\n",
    "        loss_q = qhuber(q1,tgt,self.q1.taus) + qhuber(q2,tgt,self.q2.taus)\n",
    "        self.opt_q.zero_grad(); loss_q.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q1.parameters(), GRAD_CLIP)\n",
    "        nn.utils.clip_grad_norm_(self.q2.parameters(), GRAD_CLIP)\n",
    "        self.opt_q.step()\n",
    "\n",
    "        # ---- actor(s) update ----\n",
    "        if DUAL_ACTORS:\n",
    "            # pessimistic\n",
    "            a_p , lp_p = self.pi_p(s)\n",
    "            q1_p, q2_p = self.q1.mean(s,a_p), self.q2.mean(s,a_p)\n",
    "            q_p = torch.min(q1_p,q2_p) if USE_CDQ else 0.5*(q1_p+q2_p)\n",
    "            loss_p = (alpha*lp_p - q_p).mean()\n",
    "            self.opt_p.zero_grad(); loss_p.backward(retain_graph=True); self.opt_p.step()\n",
    "\n",
    "            # optimistic\n",
    "            a_o , lp_o = self.pi_o(s)\n",
    "            q1_o, q2_o = self.q1.optimistic(s,a_o), self.q2.optimistic(s,a_o)\n",
    "            q_o = torch.min(q1_o,q2_o) if USE_CDQ else 0.5*(q1_o+q2_o)\n",
    "            loss_o = (alpha*lp_o - q_o).mean()\n",
    "            self.opt_o.zero_grad(); loss_o.backward(); self.opt_o.step()\n",
    "\n",
    "            ent = -lp_p.mean()\n",
    "        else:\n",
    "            a_pi, lp = self.pi(s)\n",
    "            q1_pi, q2_pi = self.q1.optimistic(s,a_pi), self.q2.optimistic(s,a_pi)\n",
    "            q_pi = torch.min(q1_pi,q2_pi) if USE_CDQ else 0.5*(q1_pi+q2_pi)\n",
    "            loss_pi = (alpha*lp - q_pi).mean()\n",
    "            self.opt_pi.zero_grad(); loss_pi.backward(); self.opt_pi.step()\n",
    "\n",
    "            ent = -lp.mean()\n",
    "\n",
    "        # ---- temperature ----\n",
    "        alpha_loss = (self.log_alpha.exp() * (ent - TARGET_ENT).detach()).mean()\n",
    "        self.opt_alpha.zero_grad(); alpha_loss.backward(); self.opt_alpha.step()\n",
    "\n",
    "        # ---- target polyak ----\n",
    "        with torch.no_grad():\n",
    "            for p,pt in zip(self.q1.parameters(), self.t1.parameters()):\n",
    "                pt.data.mul_(1-TAU).add_(TAU*p.data)\n",
    "            for p,pt in zip(self.q2.parameters(), self.t2.parameters()):\n",
    "                pt.data.mul_(1-TAU).add_(TAU*p.data)\n",
    "\n",
    "        # ---- periodic hard reset ----\n",
    "        self.step_ctr += 1\n",
    "        if HARD_RESET and self.step_ctr % RESET_EVERY == 0:\n",
    "            self.t1.load_state_dict(self.q1.state_dict())\n",
    "            self.t2.load_state_dict(self.q2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "referenced_widgets": [
      "620d49eff6c94c1ca83e6844b78104eb",
      "af253edc15704d8495313b1719b9ab52",
      "52cfcf75219d4363a5b8085b896cf05e",
      "bac681a3ad2b4cde9af85ef445e05a91",
      "4490201256e741f6bf22e232053c21c1",
      "eea2fbdeff484f93b0d96dc99481f45a",
      "4f50b0bd9c5b4ce0a710999e34f7c203",
      "77773226ba2f483d9547dc1c93902fe7",
      "3699dcc7b05640d7a32cbe180a8b7446",
      "fc3590de64614166901e5cca50887fdc",
      "6756a8c32e794f4c9d7dc643d3151946"
     ]
    },
    "id": "993d9534",
    "outputId": "b160b829-614f-47ab-f7bd-b0bc191ca4cc"
   },
   "outputs": [],
   "source": [
    "agent  = BRO()\n",
    "replay = Replay()\n",
    "o, _   = env.reset(seed=SEED)\n",
    "returns, steps, alpha = [], [], []\n",
    "\n",
    "for t in trange(TOTAL_STEPS, ncols=80):\n",
    "    a = env.action_space.sample() if t < START_RANDOM else agent.act(o)\n",
    "    o2, r, term, trunc, _ = env.step(a)\n",
    "    replay.add(o, a, r, o2, term or trunc)\n",
    "    o = o2 if not (term or trunc) else env.reset()[0]\n",
    "\n",
    "    if len(replay) >= START_RANDOM:\n",
    "        for _ in range(UTD):\n",
    "            agent.update(replay.sample(BATCH))\n",
    "\n",
    "    # ---------- quick eval ----------\n",
    "    if (t+1) % EVAL_EVERY == 0:\n",
    "        with torch.no_grad():\n",
    "            alpha.append(agent.log_alpha.exp().item())\n",
    "            eval_env = gym.make(ENV_ID)\n",
    "            R = 0.0\n",
    "            for _ in range(EVAL_EPISODES):\n",
    "                s,_ = eval_env.reset(seed=SEED)\n",
    "                ep_r = 0.0\n",
    "                for _ in range(eval_env.spec.max_episode_steps):\n",
    "                    s,r,done,trunc,_ = eval_env.step(agent.act(s, eval=True))\n",
    "                    ep_r += r\n",
    "                    if done or trunc: break\n",
    "                R += ep_r\n",
    "            eval_env.close()\n",
    "        mean_R = R / EVAL_EPISODES\n",
    "        returns.append(mean_R); steps.append(t+1)\n",
    "        print(f\"step {t+1}: mean eval return {mean_R:.2f}, alpha {alpha[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "32ed5422"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# reward curve\n",
    "line1 = ax1.plot(\n",
    "    steps, returns,\n",
    "    marker='o', lw=2, label=r\"mean return (50‑episode eval)\"\n",
    ")\n",
    "ax1.set_xlabel(r\"environment steps\")\n",
    "ax1.set_ylabel(r\"mean episodic return\", color=line1[0].get_color())\n",
    "ax1.tick_params(axis='y', labelcolor=line1[0].get_color())\n",
    "ax1.grid(alpha=.3)\n",
    "\n",
    "# temperature curve\n",
    "ax2 = ax1.twinx()\n",
    "line2 = ax2.plot(\n",
    "    steps, alpha,\n",
    "    marker='s', ls='--', color='tab:orange', label=r\"$\\alpha$ (entropy temperature)\"\n",
    ")\n",
    "\n",
    "# tighten alpha‑axis limits so its variations are visible\n",
    "lo, hi = min(alpha), max(alpha)\n",
    "margin = 0.1 * (hi - lo if hi > lo else 1)\n",
    "ax2.set_ylim(lo - margin, hi + margin)\n",
    "\n",
    "ax2.set_ylabel(r\"entropy temperature  $\\alpha$\", color=line2[0].get_color())\n",
    "ax2.tick_params(axis='y', labelcolor=line2[0].get_color())\n",
    "\n",
    "# legend and title\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc=\"right\")\n",
    "plt.title(f\"BRO (Fast) on {ENV_ID}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save returns as a csv file at the current folder\n",
    "path = Path.cwd() / f\"returns_{ENV_ID}.csv\"\n",
    "np.savetxt(path, np.array(returns), delimiter=\",\", header=\"mean episodic return\", comments=\"\")\n",
    "# print the path to the saved file\n",
    "print(f\"Returns saved to {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "2539ced5"
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "env = gym.make(ENV_ID, render_mode=\"rgb_array\")      # no “human” window\n",
    "vid_env = RecordVideo(env, video_folder=\"videos\",\n",
    "                      episode_trigger=lambda ep: ep == 0,\n",
    "                      disable_logger=True)\n",
    "s,_ = vid_env.reset(seed=SEED)\n",
    "done = trunc = False\n",
    "while not (done or trunc):\n",
    "    s,_,done,trunc,_ = vid_env.step(agent.act(s, eval=True))\n",
    "vid_env.close()\n",
    "\n",
    "from IPython.display import Video, HTML\n",
    "Video(sorted(Path(\"videos\").glob(\"*.mp4\"))[0].as_posix(), embed=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
